# Quick Start: Image Captioning Metrics

This is a quick reference for using the new metric calculation feature in `visualize.py`.

## What It Does

Automatically calculates BLEU, METEOR, CIDEr, and ROUGE_L metrics for captions generated by your model.

## Quick Usage

### 1. Basic Command (with metrics)

```bash
python visualize.py \
    --model log/model.pth \
    --infos_path log/infos.pkl \
    --image_folder /path/to/coco/val2014 \
    --coco_annotations coco-caption/annotations/captions_val2014.json
```

### 2. What You Get

**Console output per image:**
```
Generated caption: a bicycle with a clock as the front wheel
  BLEU-1: 0.8571, BLEU-2: 0.7143, BLEU-3: 0.5714, BLEU-4: 0.4286
  METEOR: 0.3456, CIDEr: 1.2345, ROUGE_L: 0.6789
```

**CSV file** (`evaluation_metrics.csv` in output directory):
```csv
image_id,predicted_caption,BLEU_1,BLEU_2,BLEU_3,BLEU_4,METEOR,CIDEr,ROUGE_L
391895,a bicycle with a clock as the front wheel,0.8571,0.7143,0.5714,0.4286,0.3456,1.2345,0.6789
```

## Requirements

1. **COCO images** with standard naming: `COCO_val2014_000000391895.jpg`
2. **Annotations file**: `coco-caption/annotations/captions_val2014.json`
3. **Dependencies**: pycocoevalcap (in coco-caption submodule)

### Setup Dependencies

```bash
# Clone coco-caption if not already present
git clone https://github.com/ruotianluo/coco-caption.git
```

## Metrics Explained

| Metric | Range | What it measures |
|--------|-------|------------------|
| BLEU-1 to BLEU-4 | 0-1 | N-gram overlap (higher = more word matches) |
| METEOR | 0-1 | Considers synonyms and word stems |
| CIDEr | 0-10+ | Consensus-based, weighted by importance |
| ROUGE_L | 0-1 | Longest matching sequence |

**Higher is better for all metrics.**

## Examples

### Process 10 COCO images with metrics
```bash
python visualize.py \
    --model log/model.pth \
    --infos_path log/infos.pkl \
    --image_folder data/val2014 \
    --num_images 10
```

### Process all images in a folder
```bash
python visualize.py \
    --model log/model.pth \
    --infos_path log/infos.pkl \
    --image_folder data/val2014 \
    --num_images -1
```

### Custom output location
```bash
python visualize.py \
    --model log/model.pth \
    --infos_path log/infos.pkl \
    --image_folder data/val2014 \
    --output_dir my_results \
    --num_images 5
```
The CSV will be saved as `my_results/evaluation_metrics.csv`

## Troubleshooting

### "pycocoevalcap not available"
- Clone the coco-caption repository
- Make sure matplotlib and other dependencies are installed

### "Image ID not found in COCO annotations"
- Verify your images are from COCO dataset
- Check filename format: must include image ID (e.g., `COCO_val2014_000000391895.jpg`)

### No metrics in output
- Check that `--coco_annotations` points to valid file
- Verify images are from COCO validation set
- Metrics are only calculated for valid COCO images

## Testing

### Test the feature
```bash
python test_metrics.py
```

### See a demo
```bash
python demo_metrics.py
```

## More Information

- Full documentation: `METRICS_FEATURE.md`
- Visualization guide: `VISUALIZATION.md`
- General README: `README.md`
